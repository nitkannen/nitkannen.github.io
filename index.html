<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <!-- Please delete this script if you use this HTML. -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-78552322-1"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'UA-78552322-1');
  </script>

  <meta name="viewport" content="width=500">
  <link href="stylesheet.css" rel="stylesheet" type="text/css">
  <link rel="icon" type="image/png" href="images/favicon1.ico">
  <title>Nithish Kannen</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
</head>

<body>
  <table width="1000" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p align="center">
                <name style="font-size:50px;">Nithish Kannen</name>
              </p>
              <!-- About -->
              <p > Thanks for visiting. Currently, I am a researcher (AI resident) at <a https://deepmind.google/" > Google DeepMind</a>. I am interested in 
		enabling models to become natively multimodal—learning from and integrating multiple modalities as seamlessly as humans do. I am also excited by pluralistic alignment: aligning multimodal models to reflect the diverse preferences and intentions of individuals across different cultures. 
		Beyond these areas, I am always excited to explore new topics and challenges in AI research. 
		<p>
		      
		      Previously, I was with <a href="https://www.amazon.science/" > Amazon Alexa AI</a> in London working on <i>machine learning for ranking and recommendation</i>. 
		Before that, I graduated with an Integrated Masters (BS + MS) in EECS (<a href="http://www.ee.iitkgp.ac.in/" > Electrical and </a>
          	<a href="http://cse.iitkgp.ac.in/" > Computer Science Engineering </a> )
                from  <a href="http://www.iitkgp.ac.in/" > IIT Kharagpur </a>.  
		I did my Masters Thesis with <a href="https://cse.iitkgp.ac.in/~pawang//" > Prof. Pawan Goyal </a> at
		<a href="https://cnerg-iitkgp.github.io/" > CNeRG Lab </a> which is now published at <b>EMNLP 2023</b> (Findings). My Masters Thesis was awarded the <b> Best Project Award</b> amongst 2023 graduating students. 
		
	       <p>
			I have interned with <b>Amazon Science</b> in Berlin and <b>IBM Research</b> in Bangalore. I recieved the <b>Outstanding Intern Award</b> at IBM Research.
		       For more details about my interests, please refer to the Research section and my <a href="images/Nithish_Kannen_GDM_Public_resume.pdf" >resume</a>. I am always
		      on the lookout for research opportunities and collaborations and welcome comments and thoughts on my work.
		      



                 <!-- Contact details -->
                <p align=center>
                  <a href="mailto:nithishkannen@gmail.com" ><i class="fa fa-envelope"></i> Email</a> &nbsp/&nbsp
                  <a href="images/Nithish_Kannen_GDM_Public_resume.pdf" ><i class="fa fa-file-pdf-o"></i> CV</a> &nbsp/&nbsp
                  <a href="https://www.linkedin.com/in/nithish-kannen-7a7823177/" > <i class="fa fa-linkedin"></i> LinkedIn </a> &nbsp/&nbsp
                  <a href="https://github.com/nitkannen" > <i class="fa fa-github"></i> GitHub </a> &nbsp/&nbsp
                  <a href="https://scholar.google.com/citations?user=nPQMsWMAAAAJ&hl=en" target="_blank"><font size="3">Google Scholar </font></a>  
                </p>
              </p>
            </td>
            <td width="30%">
              <img width="100%" height="12%" style="border-radius: 30%;" src="images/me_UK.jpg">
            </td>
          <tr>
        </table>

        <!-- updates -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding: 20px; width: 100% ; vertical-align: middle;">
                    <heading style="font-size:25px;">News</heading>
                    <p>
                        <ul id="news" >	
			<li><span style="color: red">[Jan' 25]</span>&nbsp;Code for our recent tech report on Proactive T2I agents is released in  <a href="https://github.com/google-deepmind/proactive_t2i_agents/" >Github</a>!! Do check it out.</li>
			<li><span style="color: red">[Dec' 24]</span>&nbsp;<span style="color: orange"><b>[Paper]</b></span> We have a tech report on <a href="https://arxiv.org/abs/2412.06771" >  Proactive Agents for Text-to-Image Models under Uncertainty</a> on Arxiv! Excited to see how the community builds on this.  </li>
			<li><span style="color: red">[Dec' 24]</span>&nbsp;Attending NeurIPS to present <a href="https://www.arxiv.org/abs/2407.06863" >CUBE</a>! See you in Vancouver :)</li>
			<li><span style="color: red">[Sep' 24]</span>&nbsp;<span style="color: orange"><b>[Paper]</b></span> Our paper <b>Beyond Aesthetics: Cultural Competence in Text-to-Image Models</b> accepted to <b>NeurIPS 2024</b>!.  </li>
			<li><span style="color: red">[Sep' 24]</span>&nbsp;<span style="color: orange"><b>[Paper]</b></span> Our paper <b>Efficient Pointwise-Pairwise Learning to Rank for Recommendation</b> accepted to <b>EMNLP 2024</b> (Findings).  </li>
			<li><span style="color: red">[Sep' 24]</span>&nbsp;<span style="color: green"><b>[Talk]</b></span> Gave a talk at <a href="https://odsc.com/europe/speakers/" > ODSC Europe</a> on <b>Cultural Competence in Multimodal Models</b>   </li>
			<li><span style="color: red">[Aug' 24]</span>&nbsp;<span style="color: green"><b>[Talk]</b></span> Gave a talk at IBM Research on <b>Leveraging Complementary Strengths of Heterogenous Sources in Machine Learning</b>.  </li>
			<li><span style="color: red">[Jul' 24]</span>&nbsp;<span style="color: orange"><b>[Paper] </b></span> Checkout our new  <a href="https://www.arxiv.org/abs/2407.06863" >preprint </a> on <b>Cultural Competence in Text-to-Image Mdoels</b>. </li>	
			<li><span style="color: red">[May ' 24]</span>&nbsp; Now in <a href="https://deepmind.google/" > Google Deepmind</a> as part of the foundational research unit led by <a href="https://scholar.google.co.uk/citations?user=0uTu7fYAAAAJ&hl=en" > Zoubin Ghahramani</a>. Will be continuing my focus on Vison-Language models.
			<li><span style="color: red">[Jan ' 24]</span>&nbsp;<span style="color: green"><b>[Talk]</b></span> Gave a talk at NLU reading group on <b>Vision-Language Models: Evaluation and Cultural Lens</b> at <b>Google Research India </b>. <a href="https://drive.google.com/file/d/1M8iX1NUEbH6sKN0DhAShEHKGl4qAAXBb/view?usp=sharing" > Slides</a>.
			<li><span style="color: red">[Dec ' 23]</span>&nbsp; Serving as a reviewer for <a href="https://2024.naacl.org/" > NAACL 2024</a>
			<li><span style="color: red">[Dec ' 23]</span>&nbsp; My Masters Thesis was awarded the <span style="color: green"><b>Best Project Award </b></span> of the graduating batch in 2023!
			<li><span style="color: red">[Dec ' 23]</span>&nbsp; I will be in Singapore for <a href="https://2023.emnlp.org/" > EMNLP 2023</a> from 4th to 12th Dec. Come say hi! 
			<li><span style="color: red">[Nov ' 23]</span>&nbsp; I moved to <a href="https://research.google/" > Google Research</a> whre I would be focussing on Multi-modal AI!
			 <li><span style="color: red">[Oct ' 23]</span>&nbsp;<span style="color: orange"><b>[Paper] </b></span> Super stoked to have 2 papers accepted at <a href="https://2023.emnlp.org/" > EMNLP 2023</a>!! Details Soon. See you in Singapore.</li>
			 <li><span style="color: red">[Aug ' 23]</span>&nbsp;<span style="color: green"><b>[Talk]</b></span> Gave a talk on <b> NLP Conspiracy Theories</b> in IXT-Science Reading group at Alexa AI. </li>	 
			 <li> <span style="color: red">[Aug ' 23]</span>&nbsp; Served as a reviewer for <a https://2023.emnlp.org/" > EMNLP 2023</a></li>
			 <li> <span style="color: red">[Jun ' 23]</span>&nbsp; Joined <b>Amazon Alexa AI </b> in London/Cambridge, UK </li>
			 <li> <span style="color: red">[Apr ' 23]</span>&nbsp; Stoked to receive admission into <a href="https://lti.cs.cmu.edu/" > Language Technology Institute (LTI) </a> at <b> Carnegie Mellon University (CMU) </b> for graduate studies! </li>
			 <li> <span style="color: red">[Apr ' 23]</span>&nbsp; Checkout my <a href="https://drive.google.com/file/d/17iRl9Z5ZccvrGcB-d2vwaFLdRM05p_jJ/view?usp=sharing" > Master's Thesis </a> that got nominated for Best Thesis award. Find the presentation <a href="https://drive.google.com/file/d/1ZyHvMn9BHqTYjSyQFzpKKQxyIfBvZCZY/view?usp=sharinghere" > here </a>  </li>
			  <li> <span style="color: red">[Oct ' 22]</span>&nbsp; Scored 115/120 at TOEFL.  </li>
			  <li><span style="color: red">[Aug '22]</span>&nbsp; Gave a talk at the NLU-GE-Science (Golden Eagle) reading group at Amazon on recent counter-intuitive findings in Prompting Literature. Check out the <a href="https://github.com/nitkannen/nitkannen.github.io/blob/main/images/RG_WaywardPrompt.pdf" >slides </a> </li>
			  <li><span style="color: red">[Jun '22]</span>&nbsp; Excited to be spending a summer at <a href="https://www.amazon.science/" > Amazon Research </a> with the Alexa AI-NLU IFS team in Berlin! </li>	
			  <li><span style="color: red">[May '22]</span>&nbsp; Received <b> Student Voulunteer Award at </b><a href="https://www.2022.aclweb.org/" > ACL 2022 </a>.  </li>	
			  <blink> <li><span style="color: red"> [Apr '22]</span>&nbsp; Super stoked to have received an offer as an Applied Scientist Intern from  <a href="https://www.amazon.science/" >Amazon Science</a> <b>Berlin</b> with the <b> Alexa NLU team </b>  </li></blink> 
			  <li><span style="color: red">[Mar '22]</span>&nbsp; Checkout our new  <a href="https://arxiv.org/abs/2203.11054" >preprint </a> on Targetted Fact Extraction for Temporal KBQA </li>	
			  <li><span style="color: red">[Jan '22]</span>&nbsp; Selected to attend <a href="https://sites.google.com/view/researchweek2022/home" >Research Summer School</a> at <a href="https://research.google/locations/india/" >Google Research India </a> - one among 30 picked for the NLP track </li>	
			  <li> <blink> <span style="color: red">[Dec '21]</span>&nbsp; One paper accepted at <b>AAAI 2022 Conference</b> (SDU Workshop) </blink> </li> 
			  <li> <span style="color: red">[Dec '21]</span>&nbsp; Serving as a sub-reviewer for SDU Workshop at AAAI Conference.  </li>	
		          <li> <span style="color: red">[Nov '21]</span>&nbsp; Manuscript submitted to AAAI 2022 (SDU Worksop)  </li>		
			  <li><span style="color: red">[Nov '21]</span>&nbsp; Team finished top3 globally in Shared Task at <a href="https://sites.google.com/view/sdu-aaai22" > Scientific Document Understanding</a> at <a href="https://aaai.org/Conferences/AAAI-22/" >AAAI 2022 </a>  </li>	
                          <li><span style="color: red">[Nov '21]</span>&nbsp; Manurscript submitted to ACL 2022 Main Conference</li>
                          <li><span style="color: red">[Sept '21]</span>&nbsp; Work done in summer 2021 received <span style="color: green"><b>Outstanding Intern Award </b></span> by Director of IBM Research, India</li>
                          <li><span style="color: red">[Sept '21]</span>&nbsp; Joined as a RA at <a href="https://cnerg-iitkgp.github.io/" > CNeRG Labs </a> IITKGP</li>
                          <li><span style="color: red">[May '21]</span>&nbsp; Started Internship at <a href="https://research.ibm.com/labs/india/ " > IBM Research AI, Bangalore </a> .</li>
                        </ul>
                    </p>
                </td>
            </tr>
        </table>
        <!-- research -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading style="font-size:25px;">Research</heading>
              <p >
I am currently at <b>Google DeepMind</b>. My goal is to make multimodal models <b>pluralistic</b>: adapt to diverse population groups to ensure safety and alignment to intentions and values of a broader spectrum of humanity.
<br>
<br>
While over 200 million people actively use AI today, this represents only a fraction of the 5 billion internet users who will one day engage with it. As AI usage expands across humanity—touching people from diverse cultural and socioeconomic backgrounds—we may not yet grasp the full spectrum of use cases that will emerge.
In order for AI to be equally transformative for <i>all</i>, I believe we must invest in making models pluralistic to diverse intentions and usage contexts of different cultures.
I currently look into this from an a) evaluation lens: creating meaningful and representative benchmarks to assess inclusivity and b) inference time pluralism: steering models to align with varied human world models.
I am also interested in the fairness and safety aspects of multimodal models. I previously contributed to the <a href="https://www.indianext.co.in/project-bindi-was-launched-by-google-to-combat-social-biases-in-ai-models/" > Bindi Project</a> at Google.
I have worked with Amazon Alexa AI at Germany, Amazon Info AI in the UK, and IBM Research AI in India. I have experience in Temporal Reasoning, QA, Conversation and Dialogue, Aspect-based Sentiment Analysis, Multilingual NLP and Recommender Systems.
	        Do reach out if you are interested in any of my works and would like to chat about it. I have been fortunate to have received great guidance and mentorship from my collaborators thus far. I am happy to guide younger folks new to the space (shoot your mails to nitkan@google.com) 
              </p>
            </td>
          </tr>
        </table>

        <!-- Publications -->

 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading style="font-size:25px;">Publications</heading>
            </td>
          </tr>
		
        </table>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="35%">
              <img width="100%" src="images/qfa_img.png" alt="BookLSTM">
            </td>
            <td valign="middle" width="65%">
                <a href="https://arxiv.org/pdf/2412.06771">
                <papertitle>Proactive Agents for Multi-Turn Text-to-Image Generation Under Uncertainty</papertitle>
               </a>
              <br>
		Meera Hahn, Wenjun Zeng, <strong> Nithish Kannen </strong>, Rich Galt, Kartikeya Badola, Been Kim, Zi Wang
	 	
	      
              <br>
		    <em> Under Review at ICLR 2025 </em>

		<br>

		<a href="https://arxiv.org/pdf/2412.06771">paper</a> |    
              <a href="https://github.com/google-deepmind/proactive_t2i_agents/">code</a>
                 
              <p >User prompts for generative AI models are often underspecified, leading to sub-optimal responses. This problem is particularly evident in text-to-image (T2I) generation, where users commonly struggle to articulate their precise intent. This disconnect between the user's vision and the model's interpretation often forces users to painstakingly and repeatedly refine their prompts. To address this, we propose a design for proactive T2I agents equipped with an interface to (1) actively ask clarification questions when uncertain, and (2) present their understanding of user intent as an understandable belief graph that a user can edit. We build simple prototypes for such agents and verify their effectiveness through both human studies and automated evaluation. We observed that at least 90% of human subjects found these agents and their belief graphs helpful for their T2I workflow. Moreover, we develop a scalable automated evaluation approach using two agents, one with a ground truth image and the other tries to ask as few questions as possible to align with the ground truth. On DesignBench, a benchmark we created for artists and designers, the COCO dataset (Lin et al., 2014), and ImageInWords (Garg et al., 2024), we observed that these T2I agents were able to ask informative questions and elicit crucial information to achieve successful alignment with at least 2 times higher VQAScore (Lin et al., 2024) than the standard single-turn T2I generation  </p>
            </td>   
          </tr>
        </table> 

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="35%">
              <img width="100%" src="images/cube_img.png" alt="BookLSTM">
            </td>
            <td valign="middle" width="65%">
                <a href="https://www.arxiv.org/pdf/2407.06863">
                <papertitle>Beyond Aesthetics: Cultural Competence in Text-to-Image Models</papertitle>
               </a>
              <br>
	      <strong> Nithish Kannen </strong>,
	 	Arif Ahmad,
             	Marco Andreetto, 
              	Vinodkumar Prabhakaran,
		Utsav Prabhu,
		Adji Bousso Dieng,
		Pushpak Bhattacharyya,
		Shachi Dave
	      
              <br>
		    <em> <b>NeurIPS 2024 </b> (D&B)</em>

		<br>

		<a href="https://www.arxiv.org/pdf/2407.06863">paper</a> |    
              <a href="https://github.com/google-research-datasets/cube">code</a>
                 
              <p >Text-to-Image (T2I) models are being increasingly adopted in diverse global communities where they create visual representations of their unique cultures. Current T2I benchmarks primarily focus on faithfulness, aesthetics, and realism of generated images, overlooking the critical dimension of cultural competence. In this work, we introduce a framework to evaluate cultural competence of T2I models along two crucial dimensions: cultural awareness and cultural diversity, and present a scalable approach using a combination of structured knowledge bases and large language models to build a large dataset of cultural artifacts to enable this evaluation. In particular, we apply this approach to build CUBE (CUltural BEnchmark for Text-to-Image models), a first-of-its-kind benchmark to evaluate cultural competence of T2I models. CUBE covers cultural artifacts associated with 8 countries across different geo-cultural regions and along 3 concepts: cuisine, landmarks, and art. CUBE consists of 1) CUBE-1K, a set of high-quality prompts that enable the evaluation of cultural awareness, and 2) CUBE-CSpace, a larger dataset of cultural artifacts that serves as grounding to evaluate cultural diversity. We also introduce cultural diversity as a novel T2I evaluation component, leveraging quality-weighted Vendi score. Our evaluations reveal significant gaps in the cultural awareness of existing models across countries and provide valuable insights into the cultural diversity of T2I outputs for under-specified prompts. Our methodology is extendable to other cultural regions and concepts, and can facilitate the development of T2I models that better cater to the global population.  </p>
            </td>   
          </tr>
        </table> 

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="35%">
              <img width="100%" src="image_glimpse.png" alt="BookLSTM">
            </td>
            <td valign="middle" width="65%">
                <a href="https://arxiv.org/abs/2409.17711">
                <papertitle>Efficient Pointwise-Pairwise Learning to Rank for News Recommendation</papertitle>
               </a>
              <br>
	      <strong> Nithish Kannen </strong>,
	 	Yao Ma,
             	Gerrit van der Burg, 
              Jean Baptiste Faddoul
	      
              <br>
		    <em> <b>EMNLP 2024 </b> (Findings)</em>

		<br>

		<a href="https://arxiv.org/abs/2409.17711">paper</a> 
                 
              <p >News recommendation is a challenging task that involves personalization based on the interaction history and preferences of each user. Recent works have leveraged the power of pretrained language models (PLMs) to directly rank news items by using inference approaches that predominately fall into three categories: pointwise, pairwise, and listwise learning-to-rank. While pointwise methods offer linear inference complexity, they fail to capture crucial comparative information between items that is more effective for ranking tasks. Conversely, pairwise and listwise approaches excel at incorporating these comparisons but suffer from practical limitations: pairwise approaches are either computationally expensive or lack theoretical guarantees and listwise methods often perform poorly in practice. In this paper, we propose a novel framework for PLM-based news recommendation that integrates both pointwise relevance prediction and pairwise comparisons in a scalable manner. We present a rigorous theoretical analysis of our framework, establishing conditions under which our approach guarantees improved performance. Extensive experiments show that our approach outperforms the state-of-the-art methods on the MIND and Adressa news recommendation datasets.  </p>
            </td>   
          </tr>
        </table> 
	


	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="35%">
              <img width="100%" src="images/BOBW.png" alt="TempQA">
            </td>
            <td valign="middle" width="65%">
              <a href="https://aclanthology.org/2023.emnlp-main.287/">
                <papertitle>Best of Both Worlds: Towards Improving Temporal Knowledge Base Question Answering via Targeted Fact Extraction</papertitle>
              </a>
              <br>
	      <strong> Nithish Kannen </strong>,
             Udit Sharma, Sumit Neelam, Dinesh Khandelwal, Shajith Ikbal, Hima Karanam, L Venkata Subramaniam
              <br>
		    <em> <b>EMNLP 2023 </b> </em>
              <br>  
	            <a href="https://aclanthology.org/2023.emnlp-main.287/">paper</a> 
              <p >Temporal question answering (QA) is a special category of complex question answering task that requires reasoning over facts asserting time intervals of events. Previous works have predominately relied on Knowledge Base Question Answering (KBQA) for temporal QA. One  of the major challenges faced by these systems is their inability to retrieve all relevant facts  due to factors such as incomplete KB and entity/relation linking errors (Patidar et al., 2022). A failure to fetch even a single fact will block KBQA from computing the answer. Such cases of KB incompleteness are even more profound in the temporal context. To address this issue, we explore an interesting direction where a targeted temporal fact extraction technique is used to assist KBQA whenever it fails to retrieve temporal facts from the KB. We model the extraction problem as an open-domain question answering task using off-the-shelf language models. This way, we target to extract from textual resources those facts that failed to get retrieved from the KB. Experimental results on two temporal QA benchmarks show promising 30% & 10% relative improvements in answer  accuracies without any additional training cost.</p>
            </td>   
          </tr>
        </table> 

	
	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="35%">
              <img width="100%" src="images/CABACE.png" alt="BookLSTM">
            </td>
            <td valign="middle" width="65%">
                <a href="https://aclanthology.org/2023.findings-emnlp.807.pdf">
                <papertitle>CONTRASTE: Supervised Contrastive Pre-training With Aspect-based Prompts For Aspect Sentiment Triplet Extraction</papertitle>
               </a>
              <br>
	      Rajdeep Mukherjee,
	      <strong> Nithish Kannen </strong>,
              Saurabh Kumar Pandey,
              Pawan Goyal
	      
              <br>
		    <em> <b>EMNLP 2023 </b> (Findings)</em>

		<br>

		<a href="https://aclanthology.org/2023.findings-emnlp.807.pdf">paper</a> |    
              <a href="https://github.com/nitkannen/contraste">code</a>
                 
              <p >Existing works on Aspect Sentiment Triplet Extraction (ASTE) explicitly focus on developing more efficient fine-tuning approaches for the task. Different from these, we propose CONTRASTE, a novel pre-training strategy using CONTRastive learning to improve the performance of the downstream ASTE task. Given a sentence and its associated (aspect, opinion, sentiment) triplets, first, we design aspect-based prompts with corresponding sentiments masked. We then (pre)train an encoder-decoder architecture by applying contrastive learning on the decoder-generated aspect-aware sentiment representations of the masked terms. For fine-tuning the pre-trained model weights thus obtained, we then propose a novel multi-task approach where the base encoder-decoder framework is combined with two complementary modules, a tagging-based Opinion Term Detector, and a regression-based Triplet Count Estimator. Exhaustive experiments on four benchmark datasets and a detailed ablation study establish the importance of each of our proposed components as we achieve new state-of-the-art results for the ASTE task. We further demonstrate that our proposed pre-training scheme can improve the performance of other ABSA tasks such as Aspect Category Opinion Sentiment (ACOS) quad prediction, Target Aspect Sentiment Detection (TASD), and Aspect Extraction and Sentiment Classification (AESC).  </p>
            </td>   
          </tr>
        </table> 
	
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="35%">
              <img width="100%" src="images/CABACE.jpg" alt="CABACE Architecture">
            </td>
            <td valign="middle" width="65%">
              <a href="https://arxiv.org/pdf/2112.13237.pdf">
                <papertitle>CABACE: Injecting Character Sequence Information and Domain Knowledge for Enhanced Acronym Extraction</papertitle>
              </a>
              <br>
	      <strong> Nithish Kannen </strong>,
              Divyanshu Sheth,
              Abhranil Chandra,
	      Shubraneel Pal
              <br>
		    <em>Scientific Document Understanding (SDU) at Thirty-Sixth AAAI Conference on Artificial Intelligence <strong>(AAAI) 2022</strong> </em>
              <br>  
	            <a href="https://arxiv.org/abs/2112.13237">paper</a> |    
              <a href="https://github.com/nitkannen/BacKGProp-AAAI-22">code</a>
              <p >Acronyms and long-forms are a common sight in research documents in general, more so in documents from scientific and legal domains. Many of the acronyms used in these documents are domain-specific, and are very rarely found in normal text corpora. Owing to this, transformer-based NLP models suffer while detecting OOV (Out of Vocabulary) for acronym tokens and their performance suffers while linking acronyms to their long forms during extraction. Moreover, transformer-based models like BERT are not specialized to handle scientific and legal documents. With these points being the overarching motivation behind this work, we propose a novel framework <b>CABACE</b>: <b>C</b>haracter-<b>A</b>ware <b>B</b>ERT for <b>AC</b>ronym <b>E</b>xtraction, that takes into account character sequences in text, and is adapted to the scientific and legal domains by masked language modelling. We further use an objective function with augmented loss function, adding mask + max loss for training CABACE. Experimental results prove the superiority of the proposed framework in comparison to various baselines. Additionally, we show that the proposed framework is better suited than baseline models for zero-shot generalization to non-English languages, thus reinforcing the effectiveness of our approach. Our team BacKGProp secured the highest scores on the French dataset, second highest on Danish and Vietnamese, and third highest in the Legal English dataset on the global Codalab leaderboard for the acronym extraction shared task at SDU AAAI-22.</p>
            </td>   
          </tr>
        </table> 


	

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="35%">
              <img width="100%" src="images/photoBook.jpg" alt="BookLSTM">
            </td>
            <td valign="middle" width="65%">
              <a href="https://iopscience.iop.org/book/edit/978-0-7503-5658-9/chapter/bk978-0-7503-5658-9ch3">
                <papertitle>Smart Factories of Industry 4.0: Determination of the Effective Smartphone Position for Human Activity Recognition using Deep Learning</papertitle>
              </a>
              <br>
	      <strong> Nithish Kannen </strong>,
             Abdulhamit Subasi
              <br>
		    <em> Chapter in <strong>Advanced Signal Processing for Industry 4.0, Volume 2</strong></em>
                 
              <p > In this chapter, we present a user-dependent and independent deep learning-based approach for transportation mode recognition using smartphone sensor data. Moreover,
		comparative experiments over six deep learning models including the Convolutional and
		Recurrent Neural Networks are conducted to determine the best position of the smartphone
		for transportation mode recognition. </p>
            </td>   
          </tr>
        </table> 

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="35%">
              <img width="100%" src="images/IBM_IRL_paper.jpg" alt="TempQA">
            </td>
            <td valign="middle" width="65%">
              <a href="https://arxiv.org/abs/2203.11054">
                <papertitle>Targeted Extraction of Temporal Facts from Textual Resources for Improved Temporal Question Answering over Knowledge Bases</papertitle>
              </a>
              <br>
	      <strong> Nithish Kannen </strong>,
             Udit Sharma, Sumit Neelam, Dinesh Khandelwal, Shajith Ikbal, Hima Karanam, L Venkata Subramaniam
              <br>
		    <em>Preprint </em>
              <br>  
	            <a href="https://arxiv.org/abs/2203.11054">paper</a> |    
              <p >Knowledge Base Question Answering (KBQA) systems have the goal of answering complex natural language questions by reasoning over relevant facts retrieved from Knowledge Bases (KB). One of the major challenges faced by these systems is their inability to retrieve all relevant facts due to factors such as incomplete KB and entity/relation linking errors. In this paper, we address this particular challenge for systems handling a specific category of questions called temporal questions, where answer derivation involve reasoning over facts asserting point/intervals of time for various events. We propose a novel approach where a targeted temporal fact extraction technique is used to assist KBQA whenever it fails to retrieve temporal facts from the KB. We use λ-expressions of the questions to logically represent the component facts and the reasoning steps needed to derive the answer. This allows us to spot those facts that failed to get retrieved from the KB and generate textual queries to extract them from the textual resources in an open-domain question answering fashion. We evaluated our approach on a benchmark temporal question answering dataset considering Wikidata and Wikipedia respectively as the KB and textual resource. Experimental results show a significant ∼30\% relative improvement in answer accuracy, demonstrating the effectiveness of our approach.</p>
            </td>   
          </tr>
        </table> 



	































 























































 
















 


        <!-- Research Experience -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Selected Experiences</heading>
              <hr>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

	<tr>
            <td width="25%" align="center">
              <a href="https://research.ibm.com/labs/india/">
                <img width="100%" src="images/GDM.png" alt="irl">
              </a>
            </td>
            <td width="75%" valign="center">
            <papertitle>Researcher, Google DeepMind </papertitle>
              <br>
              Collaborators:
		<a href="https://scholar.google.com/citations?user=CIZwXAcAAAAJ&hl=en" > Partha Talukdar</a>,  
		<a href="https://scholar.google.com/citations?user=05g10cQAAAAJ&hl=en" > Shachi Dave</a>
		    <a href="https://cs.stanford.edu/~vinod/" >Vinodkumar Prabhakaran</a>
		    <a href="https://ziw.mit.edu/" >Zi Wang</a>
		    <a href="https://scholar.google.com/citations?user=-U5l-M0AAAAJ&hl=en" >Marco Andreetto</a>
		

              <br>
              <p> Multimodal models, Safety, Alignment, Agents. Work published at NeurIPS 2024, ICLR 2025. 
                </p>
            </td>
          </tr>

	<tr>
            <td width="25%" align="center">
              <a href="https://research.ibm.com/labs/india/">
                <img width="100%" src="images/GDM.png" alt="irl">
              </a>
            </td>
            <td width="75%" valign="center">
            <papertitle>Applied Scientist, Amazon Alexa AI </papertitle>
              <br>
              Collaborators:
		<a href="https://scholar.google.com/citations?user=fPgSL3oAAAAJ&hl=en" > Yao Ma</a>,  
		<a href="https://scholar.google.com.pe/citations?user=KV9U5fkAAAAJ&hl=fr" >Gerrit J.J. van den Burg</a>
		    <a href="https://scholar.google.com/citations?user=aQRuXncAAAAJ&hl=en" >Jean Baptiste Faddoul</a>
		

              <br>
              <p> Machine Learning for Ranking, Recommendation. Work published at EMNLP 2024.
                </p>
            </td>
          </tr>
		
	 <tr>
            <td width="25%" align="center">
              <a href="https://www.amazon.science/">
                <img width="100%" src="images/AmazonSci.png" alt="AmzSci">
              </a>
            </td>
            <td width="75%" valign="center">
            <papertitle>Applied Scientist Intern - Amazon Alexa AI, Berlin</papertitle>
              
              <br>
              Supervisor(s):
                <a href="https://www.amazon.science/author/markus-boese" >Markus Boese </a> and <a href="https://scholar.google.com/citations?user=l4HGcbYAAAAJ&hl=en" >Caglar Tirkaz </a> (now at Apple AI)
              <br>
		    <p> Was part of the <b>Golden Eagle Science</b> team with a focus on <b>Alexa AI - Natural Language Understanding</b>. Developed a multi-task generative framework
			using Prompt Learning as a controllable utterance generator for intent classification and slot labelling. Manuscript detailing our work is under preparation for ACL 2023.
                </p>
            </td>
          </tr>
		
          <tr>
            <td width="25%" align="center">
              <a href="https://research.ibm.com/labs/india/">
                <img width="100%" src="images/IBMResearchcropped.jpg" alt="irl">
              </a>
            </td>
            <td width="75%" valign="center">
            <papertitle>Research Intern - IBM Research AI, Bangalore <span style="color: green">[ Outstanding Intern Award ]</span> </papertitle>
              <br>
              Supervisor(s):
		<a href="https://sites.google.com/site/shajithikbal//" > Shajith Ikbal</a>,  
		<a href=" https://scholar.google.com/citations?user=8HycjgoAAAAJ&hl=en  " > Hima Karanam</a> and
		<a href="https://researcher.watson.ibm.com/researcher/view.php?person=in-lvsubram" >  LV Subramaniam   </a>

              <br>
              <p> Worked with the Neuro-Symbolic AI team on targetted temporal fact extraction from textual resource to aid Complex Question Answering over Knoweledge Base
		      via Temporal Reasoning. Work published at EMNLP 2023.
		      
                </p>
            </td>
          </tr>
          <tr>
            <td width="25%" align="center">
              <a href="https://cnerg-iitkgp.github.io/">
                <img width="100%" src="images/CNERG.jpg" alt="cnerg">
              </a>
            </td>
            <td width="75%" valign="center">
            <papertitle> Research Assistant - CNERG Lab, IIT KGP </papertitle>
              <br>
              Supervisor(s):
                <a href="https://cse.iitkgp.ac.in/~pawang//" > Prof. Pawan Goyal </a>
              <br>
              <p> 1 ) Working on a Meta Learning framework for Cross Lingual Question Generation and Question Answering.
		      2) Working on a tagging free approach for Aspect Sentiment Triplet Extraction using syntactic features and Graph Neural Networks (GATs).
                </p>
            </td>
          </tr>

          <tr>
            <td width="25%" align="center">
              <a href="https://www.utu.fi/en/university/faculty-of-medicine/institute-of-biomedicine">
                <img width="100%" src="images/UoTurku.png" alt="UoTurku">
              </a>
            </td>
            <td width="75%" valign="center">
            <papertitle>Research Assistant - University of Turku, Finland</papertitle>
              <a href="https://github.com/abhra-nilIITKgp/University-of-Turku-Research-Project">(code)</a>
              <br>
              Supervisor(s):
                <a href="https://scholar.google.com/citations?user=W6FLhskAAAAJ&hl=en">Prof. Abdulhamit Subasi</a>
              <br>
              <p>Worked on classification of heart diseases using Signal Processing and Deep Learning techniques based on Stethoscope audio data.
                </p>
            </td>
          </tr>
        </table>


        <!-- Projects -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Selected Projects</heading>
              <hr>
            </td>
          </tr> 
        </table>

        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="100%" valign="center">
            <papertitle>Acronym and Long-Form Extraction from Scientific and Legal Documents of 6 Languages</papertitle> (<a href="https://github.com/nitkannen/BacKGProp-AAAI-22">code</a>)
            <br>  
            <i>Acronym Extraction Shared Task at AAAI-22 (SDU)</i> 
              <br>
              <p> Proposed a novel unified character-aware framework for Acronym and Long-Form extraction using domain specific language modelling and optimized objective function.
		  Our team secured the highest score on French, and 2nd highest score on Vietnamese, Danish and Legal English on the global 
		      <a href="https://competitions.codalab.org/competitions/34925#results"> Codalab </a> leaderboard.
            </td>
          </tr>
		
	  <tr>
            <td width="100%" valign="center">
            <papertitle>MetaQG: Improved Cross-Lingual Question Generation in Low-Resource Languages using Enhanced Meta-Learning Framework </papertitle>
            (<a href="https://drive.google.com/file/d/1AtI4--0-ZngOFgjQUpJGZQxnyjCSatDb/view?usp=sharing">report</a>)
            <br>  
            <i> Bachelor Thesis (Mid)</i> |Advisor:<a href="https://cse.iitkgp.ac.in/~pawang//" > Prof. Pawan Goyal </a>
              <br>
              <p> Proposed a novel framework for Cross Lingual question generation inspired from Meta Learning and Adversarial Training. Experimented with data augmentation 
		      techniques and robustness of QA and QG systems to context shuffling. Work planned for submission to TALIP Journal.
            </td>
          </tr>
        
          <tr>
            <td width="100%" valign="center">
            <papertitle>Multi-Agent Path Planning using Graph Theory </papertitle> (<a href="https://github.com/nitkannen/Multi-Agent-Path-Planning-MAPD-">code</a>) |  (<a href="https://github.com/nitkannen/Multi-Agent-Path-Planning-MAPD-/blob/main/report/MAPD_ReportFinalDraft.pdf">report</a>)
            <br>  
            <i>Artificial Intelligence Foundations and Applications Term Project</i> |Advisors: <a href="https://scholar.google.fi/citations?user=yq-ekN8AAAAJ&hl=en">Prof. Partha Pratim Chakrabarti</a> and <a href="http://www.ai.iitkgp.ac.in/People/profile/?name=arijit">Prof. Arijit Mondal</a>
              <br>
              <p> Proposed a novel Multi‑Agent Path Finding Algorithm to perform a set of pickup‑delivery tasks in a pre‑defined warehouse map using Multi‑Label A*. 
		  Performed agent‑task pair scheduling using IDA* algorithm and implemented Floyd Warshall for computing heuristics on the implicit graph   
            </td>
          </tr>
        </table>

	<!-- References -->
<!-- 	
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading style="font-size:25px;">References</heading>
            </td>
          </tr>
	</table>
		
	<table width="100%" align="center" border="0" cellpadding="20">
		
	<tr>
              <strong>Dr. Shajith Ikbal</strong> 
		<br>
              <a href="https://sites.google.com/site/shajithikbal" target="_blank">Senior Research Scientist, IBM Research AI</a>
              
          </tr>
		<br><br>
		
	<tr>
              <strong>Prof. Pawan Goyal </strong> 
		<br>
              <a href="https://cse.iitkgp.ac.in/~pawang/" target="_blank">Associate Professor, Dept. of CSE, IIT Kharagpur</a>
              
          </tr>
		<br><br>
		
	<tr>
              <strong>Prof. Abdulhamit Subasi </strong> 
		<br>
              <a href="https://scholar.google.com/citations?user=W6FLhskAAAAJ&hl=en" target="_blank">Professor, University of Turku, Finland</a>
              
         </tr> 

		
		
	
        </table>
-->

        <!-- Service -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Positions Of Responsibility & Extracurricular Activities</heading>
              <hr/>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
		
	  <tr>
            <td width="25%">
              <a href="https://iit-techambit.in/" target="_blank"><img width="75%" src="images/techambit.png" alt="NSS"></a>
            </td>
            <td width="75%" valign="center">
              <strong>Senior Editor</strong> (Apr' 20 - Jun' 21) 
	          
              <br> 
              <a href="https://iit-techambit.in/" target="_blank">IIT Tech Ambit</a> (<a href="https://iit-techambit.in/author/nithish/" target="_blank">My Profile</a>)
              <br></br>
              Official tech magazine of the IITs, developed at IIT Kharagpur that identifies research carried out by the stakeholders of IITs and their impact.
	      Authored numerous articles for monthly magazines. Interviewed stakeholders and achievers within KGP and outside           
            </td>
          </tr>
          <tr>
            <td width="25%">
              <a href="https://www.facebook.com/kgpdag/" target="_blank"><img width="75%" src="images/kdag.png" alt="kdag"></a>
            </td>
            <td width="75%" valign="center">
              <strong>Core Member</strong> (Sept '20 - Jun '21)
              <br/> 
              <a href="https://www.facebook.com/kgpdag/" target="_blank">Kharagpur Data Analytics Group (KDAG), IIT Kharagpur</a>
     	      <br/> 
              <br>
              Organized research paper-reading sessions for students of IIT Kharagpur. Conducted Data Science and ML 
              workshop for more than 600 registered students. The KDAG is a group of students enthusiastic about Data
              Science and Machine Learning, along with its applications.            
            </td>
          </tr>
          
        </table>

        <!-- Footer -->
        <table width="100%" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td class="customfsize" align="right">
              Thanks to <a class="customfsize" href="https://jonbarron.info/">Jon Barron</a> for sharing this awesome template!</i>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>
</body>
</html>
